{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import AllChem\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score, average_precision_score\n",
    "import os, joblib\n",
    "\n",
    "# import utils\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "np.random.seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# 고정된 랜덤 시드를 사용하여 재현 가능한 셔플링 설정\n",
    "g = torch.Generator()\n",
    "g.manual_seed(777)  # 고정된 시드 설정\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(777)\n",
    "    torch.cuda.manual_seed_all(777)  # 멀티 GPU 환경 시 사용\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Early Stopping을 위한 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience  # 개선되지 않는 에포크를 기다릴 수 있는 횟수\n",
    "        self.delta = delta  # 개선 기준이 되는 최소 변화량\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss):\n",
    "        \"\"\"검증 손실이 개선되었을 때 호출\"\"\"\n",
    "        self.best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.length = len(self.X)  # 데이터의 길이 저장\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.length:\n",
    "            idx = idx % self.length  # 인덱스가 데이터의 범위를 벗어나면 나머지 연산을 통해 인덱스를 조정\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(nBits, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.ReLU()(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.Sigmoid()(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.constant_(self.fc1.bias, 0)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.constant_(self.fc2.bias, 0)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.constant_(self.fc3.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, nBits, drop_rate, seed=777):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # 전역 랜덤 시드 고정\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # 모델 레이어 정의\n",
    "        self.fc1 = nn.Linear(nBits, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self._initialize_weights(seed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.ReLU()(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.Sigmoid()(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, seed):\n",
    "        torch.manual_seed(seed)  # 시드 고정\n",
    "\n",
    "        # Fully connected layers\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight, a=0, mode='fan_in', nonlinearity='linear')\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.constant_(self.fc1.bias, 0)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.constant_(self.fc2.bias, 0)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.constant_(self.fc3.bias, 0)\n",
    "\n",
    "        # BatchNorm layers\n",
    "        nn.init.constant_(self.bn1.weight, 1)\n",
    "        nn.init.constant_(self.bn1.bias, 0)\n",
    "        nn.init.constant_(self.bn2.weight, 1)\n",
    "        nn.init.constant_(self.bn2.bias, 0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
