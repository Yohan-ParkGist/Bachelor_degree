{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score, average_precision_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from utils import EarlyStopping, load_fingerprints, MLP, CustomDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "np.random.seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# 고정된 랜덤 시드를 사용하여 재현 가능한 셔플링 설정\n",
    "g = torch.Generator()\n",
    "g.manual_seed(777)  # 고정된 시드 설정\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(777)\n",
    "    torch.cuda.manual_seed_all(777)  # 멀티 GPU 환경 시 사용\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "nBits=1024\n",
    "num_epochs = 300\n",
    "k_folds=5\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cls(target_endpoint, drop_rate, lr, weight_decay, model_class):\n",
    "    # 학습 데이터 경로\n",
    "    file_path = ''\n",
    "    file_fingerprint = ''\n",
    "    data = pd.read_csv(file_path, low_memory=False)\n",
    "    fingerprints = load_fingerprints(nBits, 2, file_fingerprint)\n",
    "    \n",
    "    # 사용할 열 이름\n",
    "    target_endpoint = target_endpoint\n",
    "    \n",
    "    # Filter out rows where the target value is NaN\n",
    "    data_task = data.dropna(subset=[target_endpoint])\n",
    "    \n",
    "    # Extract the fingerprints and target values\n",
    "    X_morgan_np = fingerprints[data_task.index]  # 필요한 인덱스에 맞게 fingerprint 선택\n",
    "    y = np.array(data_task[target_endpoint])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_morgan_np, y, test_size=0.1, shuffle=True, random_state=42)\n",
    "    \n",
    "    acc_scores, precision_scores, recall_scores, f1_scores, roc_auc_scores, pr_auc_scores = [], [], [], [], [], []\n",
    "    # Test 성능 기록 리스트 추가\n",
    "    test_acc_scores, test_precision_scores, test_recall_scores, test_f1_scores, test_roc_auc_scores, test_pr_auc_scores = [], [], [], [], [], []\n",
    "    \n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f'Fold {fold + 1}/{k_folds}')\n",
    "        \n",
    "        # kfold index를 이용해서 train data와 val data의 분리\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "        # 사용자 정의 데이터셋\n",
    "        train_dataset = CustomDataset(X_train_fold, y_train_fold)\n",
    "        val_dataset = CustomDataset(X_val_fold, y_val_fold)\n",
    "        test_dataset = CustomDataset(X_test, y_test)\n",
    "    \n",
    "        # 데이터로더 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "        model = model_class(nBits=nBits, drop_rate=drop_rate).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "        # Early stopping 초기화\n",
    "        early_stopping = EarlyStopping(patience=patience, delta=0.001)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs.to(device))\n",
    "                loss = criterion(outputs, labels.to(device).float().unsqueeze(1))  # labels 크기 맞춤\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).float().unsqueeze(1)  # labels 크기 맞춤\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # 예측값 수집\n",
    "                    val_preds.extend(outputs.cpu().numpy().flatten())\n",
    "                    val_targets.extend(labels.cpu().numpy().flatten())\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {val_loss}, Train Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "            # Early Stopping 체크\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "        # Validation 성능 지표 계산\n",
    "        y_true = np.array(val_targets).flatten()\n",
    "        y_pred = np.array(val_preds).flatten()\n",
    "    \n",
    "        # Test 성능 평가\n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        test_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:  # Test loader는 고정된 데이터로 평가\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)  # labels 크기 맞춤\n",
    "                outputs = model(inputs)\n",
    "                test_preds.extend(outputs.cpu().numpy())\n",
    "                test_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "        # Test 성능 지표 계산\n",
    "        test_y_true = np.array(test_targets).flatten()\n",
    "        test_y_pred = np.array(test_preds).flatten()\n",
    "    \n",
    "        test_acc_scores.append(accuracy_score(test_y_true, (test_y_pred > 0.5).astype(int)))\n",
    "        test_precision_scores.append(precision_score(test_y_true, (test_y_pred > 0.5).astype(int)))\n",
    "        test_recall_scores.append(recall_score(test_y_true, (test_y_pred > 0.5).astype(int)))\n",
    "        test_f1_scores.append(f1_score(test_y_true, (test_y_pred > 0.5).astype(int)))\n",
    "        test_roc_auc_scores.append(roc_auc_score(test_y_true, test_y_pred))\n",
    "        test_pr_auc_scores.append(average_precision_score(test_y_true, test_y_pred))\n",
    "    \n",
    "        # # 각 fold별 Test 성능 출력\n",
    "        # print(f\"Fold {fold + 1} Test Results:\")\n",
    "        # print(f\"ROC_AUC: {test_roc_auc_scores:.4f}, PR_AUC: {test_pr_auc_scores:.4f}, ACC: {test_acc_scores:.4f}, Precision: {test_precision_scores:.4f}, Recall: {test_recall_scores:.4f}, F1: {test_f1_scores:.4f}\\n\")\n",
    "    \n",
    "        # Valid 성능 지표 계산\n",
    "        y_true = np.array(val_targets).flatten()\n",
    "        y_pred = np.array(val_preds).flatten()\n",
    "    \n",
    "        acc_scores.append(accuracy_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        precision_scores.append(precision_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        recall_scores.append(recall_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        f1_scores.append(f1_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        roc_auc_scores.append(roc_auc_score(y_true, y_pred))\n",
    "        pr_auc_scores.append(average_precision_score(y_true, y_pred))\n",
    "    \n",
    "    # Cross-validation 점수의 평균 계산\n",
    "    acc_mean, acc_std = np.mean(acc_scores), np.std(acc_scores)\n",
    "    precision_mean, precision_std = np.mean(precision_scores), np.std(precision_scores)\n",
    "    recall_mean, recall_std = np.mean(recall_scores), np.std(recall_scores)\n",
    "    f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
    "    roc_auc_mean, roc_auc_std = np.mean(roc_auc_scores), np.std(roc_auc_scores)\n",
    "    pr_auc_mean, pr_auc_std = np.mean(pr_auc_scores), np.std(pr_auc_scores)\n",
    "    \n",
    "    print(f\"Mean ROC_AUC: {roc_auc_mean:.4f} ± {roc_auc_std:.4f}\")\n",
    "    print(f\"Mean PR_AUC: {pr_auc_mean:.4f} ± {pr_auc_std:.4f}\")\n",
    "    print(f\"Mean ACC: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "    print(f\"Mean Precision: {precision_mean:.4f} ± {acc_std:.4f}\")\n",
    "    print(f\"Mean Recall: {recall_mean:.4f} ± {precision_std:.4f}\")\n",
    "    print(f\"Mean F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    \n",
    "    # Test Set 점수의 평균 계산\n",
    "    test_acc_mean, test_acc_std = np.mean(test_acc_scores), np.std(test_acc_scores)\n",
    "    test_precision_mean, test_precision_std = np.mean(test_precision_scores), np.std(test_precision_scores)\n",
    "    test_recall_mean, test_recall_std = np.mean(test_recall_scores), np.std(test_recall_scores)\n",
    "    test_f1_mean, test_f1_std = np.mean(test_f1_scores), np.std(test_f1_scores)\n",
    "    test_roc_auc_mean, test_roc_auc_std = np.mean(test_roc_auc_scores), np.std(test_roc_auc_scores)\n",
    "    test_pr_auc_mean, test_pr_auc_std = np.mean(test_pr_auc_scores), np.std(test_pr_auc_scores)\n",
    "    \n",
    "    # 전체 Test 결과 출력\n",
    "    print(\"\\nFinal Test Set Results (Mean ± Std):\")\n",
    "    print(f\"Mean ROC_AUC: {test_roc_auc_mean:.4f} ± {test_roc_auc_std:.4f}\")\n",
    "    print(f\"Mean PR_AUC: {test_pr_auc_mean:.4f} ± {test_pr_auc_std:.4f}\")\n",
    "    print(f\"Mean ACC: {test_acc_mean:.4f} ± {test_acc_std:.4f}\")\n",
    "    print(f\"Mean Precision: {test_precision_mean:.4f} ± {test_acc_std:.4f}\")\n",
    "    print(f\"Mean Recall: {test_recall_mean:.4f} ± {test_precision_std:.4f}\")\n",
    "    print(f\"Mean F1: {test_f1_mean:.4f} ± {test_f1_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_reg(target_endpoint, drop_rate, lr, weight_decay, model_class):\n",
    "    # 학습 데이터 경로\n",
    "    file_path = '/DAS_Storage4/Federate_learning/Processed_data/DeepPK_merge/Deeppk_merged_ADMET_fdamdd_merged.csv'\n",
    "    file_fingerprint = '/DAS_Storage4/yohan/ADMET'\n",
    "    data = pd.read_csv(file_path, low_memory=False)\n",
    "    fingerprints = load_fingerprints(nBits, 2, file_fingerprint)\n",
    "    \n",
    "    # 사용할 열 이름\n",
    "    target_endpoint = target_endpoint\n",
    "    \n",
    "    # Filter out rows where the target value is NaN\n",
    "    data_task = data.dropna(subset=[target_endpoint])\n",
    "    \n",
    "    # Extract the fingerprints and target values\n",
    "    X_morgan_np = fingerprints[data_task.index]  # 필요한 인덱스에 맞게 fingerprint 선택\n",
    "    y = np.array(data_task[target_endpoint])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_morgan_np, y, test_size=0.1, shuffle=True, random_state=42)\n",
    "    \n",
    "    r2_scores, mse_scores, rmse_scores, mae_scores, mape_scores, pcc_scores = [], [], [], [], [], []\n",
    "    # Test 성능 기록 리스트 추가\n",
    "    test_r2_scores, test_mse_scores, test_rmse_scores, test_mae_scores, test_mape_scores, test_pcc_scores = [], [], [], [], [], []\n",
    "    \n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f'Fold {fold + 1}/{k_folds}')\n",
    "        \n",
    "        # kfold index를 이용해서 train data와 val data의 분리\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "        # 사용자 정의 데이터셋\n",
    "        train_dataset = CustomDataset(X_train_fold, y_train_fold)\n",
    "        val_dataset = CustomDataset(X_val_fold, y_val_fold)\n",
    "        test_dataset = CustomDataset(X_test, y_test)\n",
    "    \n",
    "        # 데이터로더 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "        model = model_class(nBits=nBits, drop_rate=drop_rate).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss().to(device)\n",
    "        \n",
    "        # Early stopping 초기화\n",
    "        early_stopping = EarlyStopping(patience=patience, delta=0.001)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs.to(device))\n",
    "                loss = criterion(outputs, labels.to(device).float().unsqueeze(1))  # labels 크기 맞춤\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).float().unsqueeze(1)  # labels 크기 맞춤\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # 예측값 수집\n",
    "                    val_preds.extend(outputs.cpu().numpy())\n",
    "                    val_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {val_loss}, Train Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "            # Early Stopping 체크\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "        # Validation 성능 지표 계산\n",
    "        y_true = np.array(val_targets).flatten()\n",
    "        y_pred = np.array(val_preds).flatten()\n",
    "    \n",
    "        # Test 성능 평가\n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        test_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:  # Test loader는 고정된 데이터로 평가\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)  # labels 크기 맞춤\n",
    "                outputs = model(inputs)\n",
    "                test_preds.extend(outputs.cpu().numpy())\n",
    "                test_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "        # Test 성능 지표 계산\n",
    "        test_y_true = np.array(test_targets).flatten()\n",
    "        test_y_pred = np.array(test_preds).flatten()\n",
    "    \n",
    "        test_r2 = r2_score(test_y_true, test_y_pred)\n",
    "        test_mse = mean_squared_error(test_y_true, test_y_pred)\n",
    "        test_rmse = root_mean_squared_error(test_y_true, test_y_pred)\n",
    "        test_mae = mean_absolute_error(test_y_true, test_y_pred)\n",
    "        test_mape = mean_absolute_percentage_error(test_y_true, test_y_pred)\n",
    "        test_pcc, _ = pearsonr(test_y_true, test_y_pred)\n",
    "    \n",
    "        # 결과 저장\n",
    "        test_r2_scores.append(test_r2)\n",
    "        test_mse_scores.append(test_mse)\n",
    "        test_rmse_scores.append(test_rmse)\n",
    "        test_mae_scores.append(test_mae)\n",
    "        test_mape_scores.append(test_mape)\n",
    "        test_pcc_scores.append(test_pcc)\n",
    "    \n",
    "        # 각 fold별 Test 성능 출력\n",
    "        print(f\"Fold {fold + 1} Test Results:\")\n",
    "        print(f\"R^2: {test_r2:.4f}, MSE: {test_mse:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.4f}, PCC: {test_pcc:.4f}\\n\")\n",
    "    \n",
    "        # Valid 성능 지표 계산\n",
    "        y_true = np.array(val_targets).flatten()\n",
    "        y_pred = np.array(val_preds).flatten()\n",
    "    \n",
    "        r2_scores.append(r2_score(y_true, y_pred))\n",
    "        mse_scores.append(mean_squared_error(y_true, y_pred))\n",
    "        rmse_scores.append(root_mean_squared_error(y_true, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_true, y_pred))\n",
    "        mape_scores.append(mean_absolute_percentage_error(y_true, y_pred))\n",
    "        pcc, _ = pearsonr(y_true, y_pred)\n",
    "        pcc_scores.append(pcc)\n",
    "    \n",
    "    # Cross-validation 점수의 평균 계산\n",
    "    r2_mean, r2_std = np.mean(r2_scores), np.std(r2_scores)\n",
    "    mse_mean, mse_std = np.mean(mse_scores), np.std(mse_scores)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_scores), np.std(rmse_scores)\n",
    "    mae_mean, mae_std = np.mean(mae_scores), np.std(mae_scores)\n",
    "    mape_mean, mape_std = np.mean(mape_scores), np.std(mape_scores)\n",
    "    pcc_mean, pcc_std = np.mean(pcc_scores), np.std(pcc_scores)\n",
    "    \n",
    "    print(f\"Mean R^2: {r2_mean:.4f} ± {r2_std:.4f}\")\n",
    "    print(f\"Mean PCC: {pcc_mean:.4f} ± {pcc_std:.4f}\")\n",
    "    print(f\"Mean MSE: {mse_mean:.4f} ± {mse_std:.4f}\")\n",
    "    print(f\"Mean RMSE: {rmse_mean:.4f} ± {rmse_std:.4f}\")\n",
    "    print(f\"Mean MAE: {mae_mean:.4f} ± {mae_std:.4f}\")\n",
    "    print(f\"Mean MAPE: {mape_mean:.4f} ± {mape_std:.4f}\")\n",
    "    \n",
    "    \n",
    "    # Test Set 점수의 평균 계산\n",
    "    test_r2_mean, test_r2_std = np.mean(test_r2_scores), np.std(test_r2_scores)\n",
    "    test_mse_mean, test_mse_std = np.mean(test_mse_scores), np.std(test_mse_scores)\n",
    "    test_rmse_mean, test_rmse_std = np.mean(test_rmse_scores), np.std(test_rmse_scores)\n",
    "    test_mae_mean, test_mae_std = np.mean(test_mae_scores), np.std(test_mae_scores)\n",
    "    test_mape_mean, test_mape_std = np.mean(test_mape_scores), np.std(test_mape_scores)\n",
    "    test_pcc_mean, test_pcc_std = np.mean(test_pcc_scores), np.std(test_pcc_scores)\n",
    "    \n",
    "    # 전체 Test 결과 출력\n",
    "    print(\"\\nFinal Test Set Results (Mean ± Std):\")\n",
    "    print(f\"R^2: {test_r2_mean:.4f} ± {test_r2_std:.4f}\")\n",
    "    print(f\"PCC: {test_pcc_mean:.4f} ± {test_pcc_std:.4f}\")\n",
    "    print(f\"MSE: {test_mse_mean:.4f} ± {test_mse_std:.4f}\")\n",
    "    print(f\"RMSE: {test_rmse_mean:.4f} ± {test_rmse_std:.4f}\")\n",
    "    print(f\"MAE: {test_mae_mean:.4f} ± {test_mae_std:.4f}\")\n",
    "    print(f\"MAPE: {test_mape_mean:.4f} ± {test_mape_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cls(\n",
    "    target_endpoint='BBB_logbb(cls)',\n",
    "    drop_rate = 0.2,\n",
    "    lr = 0.01,\n",
    "    weight_decay = 0.001,\n",
    "    model_class=MLP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_reg(\n",
    "    target_endpoint='Lipophilicity', \n",
    "    drop_rate=0.1,\n",
    "    lr = 1e-3,\n",
    "    weight_decay = 1e-3,\n",
    "    model_class=MLP\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
