{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# 하이퍼파라미터 후보 설정\n",
    "lr_list = [1e-4, 1e-3, 1e-2]\n",
    "weight_decay_list = [1e-4, 1e-3, 1e-2]\n",
    "drop_rate_list = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "# 모든 조합 생성\n",
    "param_combinations = list(product(lr_list, weight_decay_list, drop_rate_list))\n",
    "\n",
    "# 결과 저장\n",
    "results = []\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for params in param_combinations:\n",
    "    lr, weight_decay, drop_rate = params\n",
    "    print(f\"Testing params: lr={lr}, weight_decay={weight_decay}, drop_rate={drop_rate}\")\n",
    "    \n",
    "    acc_scores, precision_scores, recall_scores, f1_scores, roc_auc_scores, pr_auc_scores = [], [], [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f'Fold {fold + 1}/{k_folds}')\n",
    "        \n",
    "        # kfold index를 이용해서 train data와 val data의 분리\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # 사용자 정의 데이터셋\n",
    "        train_dataset = CustomDataset(X_train_fold, y_train_fold)\n",
    "        val_dataset = CustomDataset(X_val_fold, y_val_fold)\n",
    "\n",
    "        # 데이터로더 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        # 모델 생성\n",
    "        model = MLP(nBits, drop_rate).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "        # Early stopping 초기화\n",
    "        early_stopping = EarlyStopping(patience=patience, delta=0.001)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs.to(device))\n",
    "                loss = criterion(outputs, labels.to(device).float().unsqueeze(1))  # labels 크기 맞춤\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).float().unsqueeze(1)  # labels 크기 맞춤\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # 예측값 수집\n",
    "                    val_preds.extend(outputs.cpu().numpy().flatten())\n",
    "                    val_targets.extend(labels.cpu().numpy().flatten())\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss}\")\n",
    "\n",
    "            # Early Stopping 체크\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # 성능 지표 계산\n",
    "        y_true = np.array(val_targets).flatten()\n",
    "        y_pred = np.array(val_preds).flatten()\n",
    "\n",
    "        acc_scores.append(accuracy_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        precision_scores.append(precision_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        recall_scores.append(recall_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        f1_scores.append(f1_score(y_true, (y_pred > 0.5).astype(int)))\n",
    "        roc_auc_scores.append(roc_auc_score(y_true, y_pred))\n",
    "        pr_auc_scores.append(average_precision_score(y_true, y_pred))\n",
    "\n",
    "    # Cross-validation 평균 점수 계산\n",
    "    acc_mean = np.mean(acc_scores)\n",
    "    precision_mean = np.mean(precision_scores)\n",
    "    recall_mean = np.mean(recall_scores)\n",
    "    f1_mean = np.mean(f1_scores)\n",
    "    roc_auc_mean = np.mean(roc_auc_scores)\n",
    "    pr_auc_mean = np.mean(pr_auc_scores)\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'drop_rate': drop_rate,\n",
    "        'acc_mean': acc_mean,\n",
    "        'precision_mean': precision_mean,\n",
    "        'recall_mean': recall_mean,\n",
    "        'f1_mean': f1_mean,\n",
    "        'roc_auc_mean': roc_auc_mean,\n",
    "        'pr_auc_mean': pr_auc_mean,\n",
    "    })\n",
    "\n",
    "# ROC AUC를 기준으로 최적의 하이퍼파라미터 선택\n",
    "best_result = max(results, key=lambda x: x['roc_auc_mean'])\n",
    "\n",
    "print(f\"Best params based on ROC AUC: {best_result}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
