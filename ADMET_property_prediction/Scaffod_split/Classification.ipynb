{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score, average_precision_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from scipy.stats import pearsonr\n",
    "from utils import EarlyStopping, load_fingerprints, MLP, CustomDataset\n",
    "from deepchem.data import NumpyDataset\n",
    "from deepchem.splits import ScaffoldSplitter\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "np.random.seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# 고정된 랜덤 시드를 사용하여 재현 가능한 셔플링 설정\n",
    "g = torch.Generator()\n",
    "g.manual_seed(777)  # 고정된 시드 설정\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(777)\n",
    "    torch.cuda.manual_seed_all(777)  # 멀티 GPU 환경 시 사용\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "nBits=1024\n",
    "num_epochs = 300\n",
    "k_folds=5\n",
    "patience = 10\n",
    "\n",
    "file_path = ''\n",
    "file_fingerprint = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification 성능 지표 계산 함수\n",
    "def calculate_metrics(y_true, y_pred, threshold=0.5):\n",
    "    # y_pred: 확률값, threshold 적용하여 클래스 예측\n",
    "    y_pred_class = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred_class)\n",
    "    precision = precision_score(y_true, y_pred_class)\n",
    "    recall = recall_score(y_true, y_pred_class)\n",
    "    f1 = f1_score(y_true, y_pred_class)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    pr_auc = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"PR AUC\": pr_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate=0.2\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-3\n",
    "\n",
    "# 학습 데이터 로드\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "fingerprints = load_fingerprints(nBits, 2, file_fingerprint)\n",
    "\n",
    "target_endpoint = 'BBB_logbb(cls)'\n",
    "smiles_column = 'st_smiles'  # SMILES가 저장된 열 이름\n",
    "\n",
    "# NaN 제거 및 데이터 준비\n",
    "data_task = data.dropna(subset=[target_endpoint, smiles_column])\n",
    "X_morgan_np = fingerprints[data_task.index]\n",
    "y = np.array(data_task[target_endpoint])\n",
    "smiles = data_task[smiles_column].tolist()\n",
    "\n",
    "# DeepChem NumpyDataset 생성 (ids에 SMILES 추가)\n",
    "dataset = NumpyDataset(X_morgan_np, y, ids=smiles)\n",
    "\n",
    "# Scaffold Splitter 초기화\n",
    "splitter = ScaffoldSplitter()\n",
    "\n",
    "# Scaffold Split 적용 (Train:Test 비율: 9:1)\n",
    "train_dataset, test_dataset = splitter.train_test_split(dataset=dataset, frac_train=0.9, seed=42)\n",
    "\n",
    "# 5개의 Scaffold Split 기반 Train-Valid 조합 생성\n",
    "train_valid_folds = generate_scaffold_splits(train_dataset, n_splits=5, seed=42)\n",
    "\n",
    "# Test 데이터 접근\n",
    "X_test, y_test = test_dataset.X, test_dataset.y\n",
    "\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Train-Valid Fold 접근 방법\n",
    "for i, (train_fold, valid_fold) in enumerate(train_valid_folds):\n",
    "    print(f\"Fold {i + 1}: Train size = {len(train_fold)}, Valid size = {len(valid_fold)}\")\n",
    "    X_train_fold, y_train_fold = train_fold.X, train_fold.y\n",
    "    X_valid_fold, y_valid_fold = valid_fold.X, valid_fold.y\n",
    "\n",
    "# 5개의 Train-Valid Fold에 대해 학습 및 테스트\n",
    "fold_results = []\n",
    "valid_results = []\n",
    "\n",
    "for fold_idx, (train_fold, valid_fold) in enumerate(train_valid_folds):\n",
    "    print(f\"\\n=== Fold {fold_idx + 1} ===\")\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = CustomDataset(train_fold.X, train_fold.y)\n",
    "    val_dataset = CustomDataset(valid_fold.X, valid_fold.y)\n",
    "    test_dataset = CustomDataset(X_test, y_test)\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # 모델, 옵티마이저, 손실 함수 초기화\n",
    "    model = MLP(nBits, drop_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss\n",
    "    \n",
    "    # Early stopping 초기화\n",
    "    early_stopping = EarlyStopping(patience=patience, delta=0.001)\n",
    "    \n",
    "    # 학습\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device).float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy().flatten())\n",
    "                val_targets.extend(labels.cpu().numpy().flatten())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}, Train Loss: {running_loss / len(train_loader):.4f}')\n",
    "        \n",
    "        # Early stopping 체크\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Validation 성능 계산\n",
    "    val_metrics = calculate_metrics(np.array(val_targets), np.array(val_preds))\n",
    "    valid_results.append(val_metrics)\n",
    "    print(f\"Validation Metrics (Fold {fold_idx + 1}): {val_metrics}\")\n",
    "    \n",
    "    # Test Step\n",
    "    model.eval()\n",
    "    test_preds, test_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            test_preds.extend(outputs.cpu().numpy().flatten())\n",
    "            test_targets.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    # Test 성능 계산\n",
    "    test_metrics = calculate_metrics(np.array(test_targets), np.array(test_preds))\n",
    "    fold_results.append(test_metrics)\n",
    "    print(f\"Test Metrics (Fold {fold_idx + 1}): {test_metrics}\")\n",
    "\n",
    "# Test 결과 평균 및 표준편차 출력\n",
    "metrics_keys = fold_results[0].keys()\n",
    "summary_results = {key: [] for key in metrics_keys}\n",
    "\n",
    "for result in fold_results:\n",
    "    for key, value in result.items():\n",
    "        summary_results[key].append(value)\n",
    "\n",
    "# 평균 및 표준편차 계산\n",
    "print(\"\\n=== Test Results Summary ===\")\n",
    "for key, values in summary_results.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"{key}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "# Validation 결과 평균 출력\n",
    "print(\"\\n=== Validation Results Summary ===\")\n",
    "for key in valid_results[0].keys():\n",
    "    mean_val = np.mean([res[key] for res in valid_results])\n",
    "    std_val = np.std([res[key] for res in valid_results])\n",
    "    print(f\"{key}: {mean_val:.4f} ± {std_val:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
